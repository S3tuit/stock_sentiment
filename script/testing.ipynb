{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from script.helper.models import Article\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai version: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import confluent_kafka\n",
    "\n",
    "\n",
    "print(f\"openai version: {confluent_kafka.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEEKING ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3f7b06e7a8msh390b7f13312554ep1ecb05jsnbae1915ac3c3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Find them at https://rapidapi.com/apidojo/api/seeking-alpha/playground\n",
    "API_KEY = os.environ['SEEKING_ALPHA_API_KEY']\n",
    "API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peppa\n",
      "pig\n",
      "papa\n"
     ]
    }
   ],
   "source": [
    "diz = {'peppa': [1,2], 'pig': [3,4], 'papa': [5,6]}\n",
    "\n",
    "for i in diz:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from script.helper.models import Article\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Find them at https://rapidapi.com/apidojo/api/seeking-alpha/playground\n",
    "API_KEY = os.environ['SEEKING_ALPHA_API_KEY']\n",
    "API_HOST = \"seeking-alpha.p.rapidapi.com\"\n",
    "\n",
    "# Retrives the article content async\n",
    "async def fetch_seeking_alpha(session, id, timestp, title, ticket):\n",
    "    url = \"https://seeking-alpha.p.rapidapi.com/news/get-details\"\n",
    "    querystring = {\"id\": id}\n",
    "    headers = {\n",
    "        \"x-rapidapi-key\": API_KEY,\n",
    "        \"x-rapidapi-host\": API_HOST\n",
    "    }\n",
    "\n",
    "    async with session.get(url, headers=headers, params=querystring) as response:\n",
    "        if response.status == 200:\n",
    "            data = await response.json()\n",
    "            \n",
    "            # This extract just the content without the html components\n",
    "            soup = BeautifulSoup(data['data']['attributes']['content'], 'html.parser')\n",
    "            article_content = soup.get_text()\n",
    "\n",
    "            return Article(\n",
    "                ticket=ticket,\n",
    "                timestp=timestp,\n",
    "                url=data['data']['links']['canonical'],\n",
    "                title=title,\n",
    "                article_body=article_content\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            print(f\"Failed to fetch details for ID {id}, status code {response.status}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Get a list of article links about a specific stock\n",
    "async def seeking_alpha_get_links(ticket, num):\n",
    "    \n",
    "    # For details https://rapidapi.com/apidojo/api/seeking-alpha/playground\n",
    "    url = \"https://seeking-alpha.p.rapidapi.com/news/v2/list-by-symbol\"\n",
    "    querystring = {\"size\": num, \"number\": \"1\", \"id\": ticket}\n",
    "    headers = {\n",
    "        \"x-rapidapi-key\": API_KEY,\n",
    "        \"x-rapidapi-host\": API_HOST\n",
    "    }\n",
    "    \n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url, headers=headers, params=querystring) as response:\n",
    "            if response.status == 200:\n",
    "                data = await response.json()\n",
    "                tasks = []\n",
    "                \n",
    "                # iterates for each entity, defined by the parameter \"num\"\n",
    "                for row in data['data']:\n",
    "                    \n",
    "                    # Convert the date to unix\n",
    "                    date_str = row['attributes']['publishOn']\n",
    "                    date_time = datetime.fromisoformat(date_str)\n",
    "                    unix_timestamp = int(date_time.timestamp())\n",
    "                    \n",
    "                    # Prerate all the tasks to run async\n",
    "                    task = fetch_seeking_alpha(\n",
    "                        session=session,\n",
    "                        id=row['id'],\n",
    "                        timestp=unix_timestamp,\n",
    "                        title=row['attributes']['title'],\n",
    "                        ticket=ticket\n",
    "                    )\n",
    "                    tasks.append(task)\n",
    "\n",
    "                # Instanciate a list of Article entities\n",
    "                articles: List[Article] = []\n",
    "                \n",
    "                # Process the tasks as they complete\n",
    "                for task in asyncio.as_completed(tasks):\n",
    "                    result = await task\n",
    "                    if result:\n",
    "                        articles.append(result)\n",
    "                \n",
    "                return articles\n",
    "            \n",
    "            else:\n",
    "                print(f\"Failed to fetch list, status code {response.status}\")\n",
    "                return []\n",
    "\n",
    "articles = asyncio.run(seeking_alpha_get_links(\"MARA\", 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticket='MARA' url='https://seekingalpha.com/news/4140691-bitcoin-mining-is-a-survival-game-at-this-point-halving-didnt-do-any-favors' title='Bitcoin mining is a ‘survival game’ at this point; halving didn’t do any favors' article_body='    luza studios “Bitcoin miners are caught in a vice, and the pressure’s only intensifying. The April 2024 halving didn’t just tighten the screws; it flipped the industry on its head,” said David Materazzi, CEO of automated trading platform Galileo FX. Following the halving event, miners’ block rewards were slashed by 50%, cutting into bitcoin (BTC-USD) production and underlying revenues. And with elevated operating costs, many miners experienced dwindling profits during the quarter ended June 30, 2024. The 4% decline in bitcoin’s price during the three-month period only exacerbated the financial pressure. As such, Materazzi expects \"the weak to be picked off, leaving only the leanest operations standing tall.” While most miners are struggling to stay afloat, a few are standing out. Riot Platforms (NASDAQ:RIOT), for instance, posted its first quarterly loss since Q3 2023, as the quadrennial halving reduced available bitcoin (BTC-USD) production. Materazzi didn’t seem all that worried, though, saying Riot “is bulldozing ahead, cranking up its hash rate to a staggering 41 [exahashes per second] by 2025, all while sitting on a fat stack of cash.” Marathon Digital Holdings (NASDAQ:MARA) also turned in a wider-than-expected loss in Q2, reflecting a combination of unfavorable mark-to-market adjustment of digital assets and a decrease in bitcoin (BTC-USD) production after April’s halving. But the company still expects its 2024 hash rate to jump to 50 exahashes per second (EH/s), compared with its installed hash rate of 31.5 EH/s in Q2. In an effort to sustain cash flow stability, some miners have jumped into artificial intelligence and high-performance computing. HIVE Digital Technologies (NASDAQ:HIVE) in one of them, with its HPC segment accounting for about 8% of its total fiscal Q1 revenue. The company delivered a stronger-than-expected profit for the quarter ended June 30, 2024, as the miner navigated the halving by boosting its installed hashrate. Blake Morgan, managing partner of crypto tokenization firm Mineral Vault, noted that a handful of miners seem to be adjusting to the impacts of the halving – which typically play out over a few months – “by securing cheaper sources of energy or gaining efficiency in their operations.” Conversely, he added, “smaller miners that use waste energy are actually more successful than larger rivals that can’t cut electricity bills.\" Going forward, miners may experience relief in the upcoming quarters, as “the price of bitcoin tends to appreciate months subsequent to a halving event,” Morgan said. Other bitcoin miners: Hut 8 (NASDAQ:HUT), Sphere 3D (NASDAQ:ANY), Digihost Technology (NASDAQ:DGHI), Core Scientific (NASDAQ:CORZ), CLeanSpark (NASDAQ:CLSK), TeraWulf (NASDAQ:WULF), Iris Energy (NASDAQ:IREN), Cipher Mining (NASDAQ:CIFR), Bitfarms (NASDAQ:BITF), Greenidge Generation (NASDAQ:GREE), Stronghold Digital Mining (NASDAQ:SDIG), Bit Digital (NASDAQ:BTBT), Bitdeer Technologies (NASDAQ:BTDR).  More on the Crypto Market  Bitcoin: This Correction Is Probably Not Over Yet Will Crypto Cream Rise To The Top? HIVE Digital Technologies Ltd. (HIVE) Q1 2025 Earnings Call Transcript Crypto bill can be passed this year, Schumer says, as Democrats woo industry  ' timestp=1723925460\n",
      "ticket='MARA' url='https://seekingalpha.com/news/4141227-nycb-qifu-rise-the-most-among-financial-stocks-weekly-roundup' title='NYCB, Qifu rise the most among financial stocks: weekly roundup' article_body=\"    da-kuk Financial stocks (NYSEARCA:XLF) ended the past week on a robust footing, climbing 3.2%, as Wall Street's major equity averages delivered the strongest weekly gains so far this year, aided by economic data that eased recession fears. Rising the most of any financial stock (with market cap over $2B), Qifu Technology (NASDAQ:QFIN) surged 20.4% for the week ended Aug. 16 after the Chinese credit-tech platform turned in Q2 results and issued guidance for Q3; Regional lender New York Community Bancorp (NYSE:NYCB) jumped 18.1%; Hamilton Insurance Group (NYSE:HG) gapped up 15.6%; Japanese financial firm Nomura Holdings (NYSE:NMR) accelerated 14.5%; and Nu Holdings (NYSE:NU) gained 13.6% after the Brazilian digital bank known as Nubank posted Q2 earnings and revenue that climbed above consensus estimates as it boosted sales while also focusing on operating efficiency. For the losers, bitcoin (BTC-USD) miner Marathon Digital Holdings (NASDAQ:MARA), which during the week priced $250M of senior convertible notes, dipped 4.6% as the price of bitcoin slipped on the week; Kazakhstan-based investment bank Freedom Holding (NASDAQ:FRHC) drifted down 3.4%; Axos Financial (NYSE:AX), a regional bank, edged lower by 1.4%; India's HDFC Bank (NYSE:HDB) inched down 1%; and Ryan Specialty Holdings (NYSE:RYAN) rounded out the five biggest losers with a 0.9% loss.  More on Axos Financial, Freedom Holding, etc.  Qifu Technology: Attractive Shareholder Yield And Positive Earnings Outlook Nu Holdings: Largest Non-Asian Digital Bank, Continues To Deliver Axos Financial: When Quality Deserves A Premium Whale Rock dives into Google, Coupang, sheds Salesforce, Marvell, others in Q2 Saudi Arabia sovereign fund Q2 moves include more than tripling Nu stake, trimming Visa  \" timestp=1723903800\n"
     ]
    }
   ],
   "source": [
    "for article in articles:\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOOD FUNCTION FETCH YAHOO (no Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched for the link https://finance.yahoo.com/quote/ACMR/news/.\n",
      "Page fetched for the link https://finance.yahoo.com/news/acm-research-inc-acmr-trending-130017800.html.\n",
      "Article body found!\n",
      "Page fetched for the link https://finance.yahoo.com/news/invest-acm-research-acmr-based-133015686.html.\n",
      "Article body found!\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from script.helper.models import Article\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "# Define variables\n",
    "TICKET = 'ACMR'\n",
    "url = f'https://finance.yahoo.com/quote/{TICKET}/news/'\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36'\n",
    "}\n",
    "cookies = {\n",
    "    'GUC': 'AQABCAFmwcNm80IfWgSU&s=AQAAAGoqDQ7v&g=ZsB0FA',\n",
    "    'A1': 'd=AQABBBbSSWYCEMGNccbgJJ6fo37cGXpNRK4FEgABCAHDwWbzZudVb2UBAiAAAAcIFNJJZkCw6_E&S=AQAAAhcGLnTIGaPvJkY30Nu1ux4',\n",
    "    'A3': 'd=AQABBBbSSWYCEMGNccbgJJ6fo37cGXpNRK4FEgABCAHDwWbzZudVb2UBAiAAAAcIFNJJZkCw6_E&S=AQAAAhcGLnTIGaPvJkY30Nu1ux4',\n",
    "    'A1S': 'd=AQABBBbSSWYCEMGNccbgJJ6fo37cGXpNRK4FEgABCAHDwWbzZudVb2UBAiAAAAcIFNJJZkCw6_E&S=AQAAAhcGLnTIGaPvJkY30Nu1ux4',\n",
    "    'PRF': 't%3DACMR%26newChartbetateaser%3D0%252C1725284010825'\n",
    "}\n",
    "\n",
    "\n",
    "# Gets the text content of the page\n",
    "async def fetch_page(session, url):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            response.raise_for_status()\n",
    "            return await response.text()\n",
    "        \n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong while fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "async def fetch_yahoo_article_content(session, link, title, ticket):\n",
    "    article_content = await fetch_page(session, link)\n",
    "    print(f'Page fetched for the link {link}.')\n",
    "    \n",
    "    if article_content:\n",
    "        article_soup = BeautifulSoup(article_content, 'html.parser')\n",
    "        body = article_soup.find(class_=\"caas-body\") # This find the body of the article\n",
    "        \n",
    "        if body:\n",
    "            print('Article body found!')\n",
    "            # This return the concatenation of all the text inside the paragraphs of the article\n",
    "            article_content = \"\\n\".join(p.get_text(strip=False) for p in body.find_all('p'))\n",
    "            return Article(\n",
    "                ticket=ticket,\n",
    "                timestp=int(datetime.now().timestamp()),\n",
    "                url=link,\n",
    "                title=title,\n",
    "                article_body=article_content\n",
    "            )\n",
    "        \n",
    "    print('No article body found.')\n",
    "    return None\n",
    "\n",
    "\n",
    "async def fetch_yahoo(ticket, headers, cookies):\n",
    "        \n",
    "    url = f'https://finance.yahoo.com/quote/{TICKET}/news/'\n",
    "    async with aiohttp.ClientSession(headers=headers, cookies=cookies) as session:\n",
    "        \n",
    "        # Fetch the main page\n",
    "        page_content = await fetch_page(session, url)\n",
    "        print(f'Page fetched for the link {url}.')\n",
    "        if not page_content:\n",
    "            print('No page content found.')\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "        # Instanciate a list of Article entities\n",
    "        articles: List[Article] = []\n",
    "        tasks = []\n",
    "\n",
    "        # This locate the section on the main page with all the article links\n",
    "        for news in soup.find_all(class_='js-stream-content Pos(r)'):\n",
    "            \n",
    "            article = news.find('h3').find('a')\n",
    "            title = article.get_text(strip=True)\n",
    "            link = article.get('href')\n",
    "            \n",
    "            # Check if the url is about an add and not about the stock\n",
    "            if ticket in title:\n",
    "                tasks.append(fetch_yahoo_article_content(session, link, title, ticket))\n",
    "\n",
    "        # Process the tasks as they complete\n",
    "        for task in asyncio.as_completed(tasks):\n",
    "            result = await task\n",
    "            if result:\n",
    "                articles.append(result)\n",
    "        \n",
    "        return articles\n",
    "        \n",
    "articles = asyncio.run(fetch_yahoo(TICKET, HEADERS, cookies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAPA'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap = \"papa\"\n",
    "ap.upper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_kafka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
